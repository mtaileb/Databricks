{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73c88ccc-7236-411e-8fcf-1404ae44aa8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Création d’une table Delta et insertion initiale\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Création d'une session Spark (dans Databricks, elle est déjà disponible)\n",
    "spark = SparkSession.builder.appName(\"DeltaACIDExample\").getOrCreate()\n",
    "\n",
    "# Chemin de la table Delta\n",
    "delta_table_path = \"/tmp/delta_table_acid\"\n",
    "\n",
    "# Création de la table Delta avec des données initiales\n",
    "data = [\n",
    "    (1, \"Produit A\", 100),\n",
    "    (2, \"Produit B\", 200)\n",
    "]\n",
    "columns = [\"id\", \"product\", \"price\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Écriture initiale des données dans une table Delta\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "print(\"Table Delta initialisée avec les données suivantes :\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5929ee81-4548-4cde-bff0-8fa25810951e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Ajout de données dans la table Delta\n",
    "# Ajout de nouvelles données dans la table Delta\n",
    "new_data = [\n",
    "    (3, \"Produit C\", 300),\n",
    "    (4, \"Produit D\", 400)\n",
    "]\n",
    "\n",
    "new_df = spark.createDataFrame(new_data, columns)\n",
    "\n",
    "# Écriture des nouvelles données dans la table Delta\n",
    "new_df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n",
    "\n",
    "print(\"Données ajoutées à la table Delta :\")\n",
    "spark.read.format(\"delta\").load(delta_table_path).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "781fd4ea-9666-4177-8e30-b3cb26a662a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Mise à jour des données existantes\n",
    "# Mise à jour du prix du produit avec id=1\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Charger la table Delta\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "# Appliquer une mise à jour\n",
    "delta_table.update(\n",
    "    condition=\"id = 1\",  # Condition pour identifier la ligne à mettre à jour\n",
    "    set={\"price\": \"150\"}  # Mettre à jour la colonne price\n",
    ")\n",
    "\n",
    "print(\"Table Delta après la mise à jour :\")\n",
    "delta_table.toDF().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65542dc2-403f-476a-983e-baa95d9466e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Gestion des conflits entre plusieurs utilisateurs\n",
    "'''\n",
    "Ce scénario simule un conflit lorsque deux utilisateurs tentent de modifier les mêmes données simultanément. Delta Lake garantit que les transactions sont gérées de manière atomique.\n",
    "'''\n",
    "\n",
    "# Transaction simulée par deux utilisateurs\n",
    "\n",
    "# Utilisateur 1 : Mise à jour du produit avec id=2\n",
    "delta_table.update(\n",
    "    condition=\"id = 2\",\n",
    "    set={\"price\": \"250\"}\n",
    ")\n",
    "\n",
    "# Utilisateur 2 : Tentative d'ajout d'une ligne avec id=2 (conflit potentiel)\n",
    "try:\n",
    "    conflicting_data = [(2, \"Produit B - Modifié\", 300)]\n",
    "    conflicting_df = spark.createDataFrame(conflicting_data, columns)\n",
    "\n",
    "    # Écrire des données conflictuelles\n",
    "    conflicting_df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n",
    "except Exception as e:\n",
    "    print(\"Conflit détecté lors de l'écriture concurrente :\")\n",
    "    print(e)\n",
    "\n",
    "# Lecture finale de la table Delta\n",
    "print(\"État final de la table Delta :\")\n",
    "delta_table.toDF().show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gestion conflits",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
