{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18c80a57-d9cd-425b-b01a-6ce367bb7070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Initialisation : Création d'une table Delta\n",
    "# Créons une table Delta avec un schéma initial et insérons quelques données.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Chemin de la table Delta\n",
    "delta_table_path = \"/tmp/delta_table_schema\"\n",
    "\n",
    "# Création initiale de la table Delta\n",
    "data = [\n",
    "    (1, \"Produit A\", 100),\n",
    "    (2, \"Produit B\", 200)\n",
    "]\n",
    "columns = [\"id\", \"product\", \"price\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Écriture initiale des données\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "print(\"Table Delta initiale :\")\n",
    "spark.read.format(\"delta\").load(delta_table_path).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "061a54ba-619a-464f-9cdd-48618d23a19f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Ajout d’une colonne avec MERGE\n",
    "'''\n",
    "Ajoutons une nouvelle colonne (category) via un MERGE avec un jeu de données source. Cela nécessite l’activation de l’option mergeSchema.\n",
    "'''\n",
    "\n",
    "# Nouveaux données avec une colonne supplémentaire \"category\"\n",
    "new_data = [\n",
    "    (1, \"Produit A\", 100, \"Catégorie 1\"),\n",
    "    (3, \"Produit C\", 300, \"Catégorie 2\")\n",
    "]\n",
    "new_columns = [\"id\", \"product\", \"price\", \"category\"]\n",
    "\n",
    "new_df = spark.createDataFrame(new_data, new_columns)\n",
    "\n",
    "# Activer l'évolution automatique du schéma lors d'un MERGE\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "delta_table.alias(\"target\").merge(\n",
    "    new_df.alias(\"source\"),\n",
    "    \"target.id = source.id\"\n",
    ").whenMatchedUpdateAll(\n",
    ").whenNotMatchedInsertAll(\n",
    ").execute()\n",
    "\n",
    "print(\"Table Delta après ajout de la colonne 'category' :\")\n",
    "spark.read.format(\"delta\").load(delta_table_path).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "182df5a3-2908-479b-a178-13bb43976cd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Suppression d’une colonne avec ALTER TABLE\n",
    "# Supprimons une colonne (category) de la table Delta.\n",
    "\n",
    "# Suppression de la colonne \"category\"\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE delta.`{delta_table_path}`\n",
    "DROP COLUMN category\n",
    "\"\"\")\n",
    "\n",
    "print(\"Table Delta après suppression de la colonne 'category' :\")\n",
    "spark.read.format(\"delta\").load(delta_table_path).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce00e20e-1625-4631-95e9-85b44a01b9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Détection et gestion des incompatibilités de schéma\n",
    "# Essayons d'insérer des données avec un schéma incompatible sans activer l’option mergeSchema.\n",
    "\n",
    "# Données incompatibles (colonne supplémentaire \"discount\")\n",
    "incompatible_data = [\n",
    "    (4, \"Produit D\", 400, 10)\n",
    "]\n",
    "incompatible_columns = [\"id\", \"product\", \"price\", \"discount\"]\n",
    "\n",
    "incompatible_df = spark.createDataFrame(incompatible_data, incompatible_columns)\n",
    "\n",
    "# Tentative d'écriture sans autoriser l'évolution du schéma\n",
    "try:\n",
    "    incompatible_df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n",
    "except Exception as e:\n",
    "    print(\"Erreur détectée en raison d'un schéma incompatible :\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78caa837-f27f-4064-ae2d-f22fd967b53e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5. Résolution : Forcer l’évolution du schéma\n",
    "# Pour résoudre l’incompatibilité, utilisons l’option mergeSchema.\n",
    "\n",
    "# Réécriture avec mergeSchema pour autoriser l'évolution\n",
    "incompatible_df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(delta_table_path)\n",
    "\n",
    "print(\"Table Delta après résolution et ajout de la colonne 'discount' :\")\n",
    "spark.read.format(\"delta\").load(delta_table_path).show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Gestion des schémas",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
